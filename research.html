<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Research of Hanyuan Xiao</title>
    <meta name="description" content="Research of Hanyuan (Cornelius) Xiao">
    <meta name="keywords" content="Hanyuan,Xiao,Cornelius,Hsiao,RPI,rpi">
    <meta name="author" content="Hanyuan Xiao">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
    <!--Import Google Icon Font-->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!--Import materialize.css-->
    <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>

    <!--Let browser know website is optimized for mobile-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <!-- CSS  -->
    <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  </head>

  <body>
    <main>
      <!-- Navigation Bar -->
      <nav class="nav-extended yellow darken-4" role="navigation">
        <div class="nav-wrapper container">
          <a id="logo-container" class="brand-logo">Research</a>
          <a href="#" data-target="nav-mobile" class="sidenav-trigger"><i class="material-icons">menu</i></a>
          <ul class="right hide-on-med-and-down">
            <li><a href="index.html">Homepage</a></li>
            <li class="active"><a href="research.html">Research</a></li>
            <li><a href="coursework.html">Coursework</a></li>
            <li><a href="publication.html">Publication</a></li>
          </ul>
        </div>

        <div class="nav-content container">
          <ul class="tabs tabs-transparent">
            <li class="tab"><a class="active" href="#Overview">Overview</a></li>
            <li class="tab"><a href="#VR_SOE">VR Acquisition & Application</a></li>
            <li class="tab"><a href="#GCVA">Google Cardboard VR/AR</a></li>
          </ul>
        </div>
      </nav>

      <ul id="nav-mobile" class="sidenav">
        <li><a href="index.html">Homepage</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="coursework.html">Coursework</a></li>
        <li><a href="publication.html">Publication</a></li>
      </ul>

      <div id="Overview">
        <div class="container">
          <div class="section">
            <h4>2018</h4>
            <div class="divider"></div>
            <ul class="browser-default">
              <li>
                <h5>VR Acquisition & Application Development for School of Engineering Research</h5>
                <p>2018 Spring - Present</p>
                <p>This research investugates Photogrammetry technology for 3D reconstruction, as part of the new virtual and augmented reality lab for the School of Engineering. We are conducting research in method to raise the quality of Photogrammetry-generated 3D environments to make them as believable as possible, using large image datadets. We are also identifying common issues to develop best-practices guidelines for image acquisition in both indoor and outdoor environments, using combinations of off-the-shelf tools. One of our hypothesis is quality of work Photogrammetry generates can become reality-like if the size of image-data set is approaching to infinity. The main methodology of this research is gathering documentation from various professionals in all steps of pipeline and then analyze, select and implement in our own data set. 
                  <ul class="browser-default">
                    <li><a class="bold-text black-text">2018 Spring: </a>We have successfully captured several scenes from around the JEC building and published them in the SteamVR Workshop to allow anyone with a VR system to experience.</li>
                    <li><a class="bold-text black-text">2018 Summer: </a>We designed a program named <a class="italic-text black-text">3D-Stitching</a> to fill holes between two models, deal with mesh overlaps in between, and calculate geometry along with texture in large blank which has repeated details in real-world. We are also investigaing issue with non-planar ground surfaces to allow virtual objects to realistically interact with the generated environments.</li>
                  </ul>
                </p>
              </li>
            </ul>
          </div>
          <div class="section">
            <h4>2017</h4>
            <div class="divider"></div>
            <ul class="browser-default">
              <li><h5>Google Cardboard VR/AR</h5></li>
              <p>2017 Fall</p>
              <p>In this research, we investigated method to implement VR/AR on website and created 360 environment with basic hardware available. In order to achieve our final goal to establish a project with both front-end and back-end which supports everyone from the world to visit our environment, we set up three milestones during development. Within the development of the first one, we utilized 360 camera, Google Cardboard headset and other supported hardwares and found the optimal way to collect data from reality. Next, we studied the structure and usage of APIs of Aframe-in which is equipped with algorithm to bring common type of 360 videos, photos or environment onto website. To visualize our research, we wrote a program which provides users with free trip into RPI west campus. Users are enabled to select local destinations on campus or select direction to go inside 360 environments. In the last milestone, we further extended the program from the second milestone that can now navigate users into RPI campus with classical shortest-path algorithm and image data set collected.</p>
            </ul>
          </div>
        </div>
      </div>

      <div id="VR_SOE">
        <div class="container">
          <div class="section">
            <h4>VR Acquisition & Application</h4>
            <div class="divider"></div>
            <h5>People</h5>
            <ul class="browser-default">
              <li>Hanyuan Xiao</li>
              <li>Prof. <a href="https://www.ecse.rpi.edu/~rjradke/">Rich Radke</a>, ECSE (Advisor)</li>
            </ul>
            <h5>Abstract</h5>
            <p>
              This research investigates Photogrammetry technology for 3D model construction, as part of the new virtual and augmented reality lab for the School of Engineering. We are conducting research in method to raise the quality of Photogrammetry-generated 3D environments to make them as believable as possible, using large image data sets.  We are also identifying common issues to develop best-practices guidelines for image acquisition in both indoor and outdoor environments, using combinations of off-the-shelf tools. One of our hypothesis is quality of work Photogrammetry generates can become reality-like if the size of image-data set is approaching to infinity. The main methodology is gathering documentation from various professionals in all steps of pipeline and then analyze, select and implement in our own data set. So far, we have successfully captured several scenes from around the JEC and published them in the Steam workshop so that anyone with a VR headset can virtually experience these environments. Currently, we have started to analyzing and solving issues such as creating non-planar ground surfaces to allow virtual objects to realistically interact with the generated environments.
            </p>
            <h5>Poster</h5>
            <div class="container center"><img class="materialboxed" width="100%" src="src/VR_SOE/URS Poster.jpg"></div>
            <h5>Results</h5>
            <ul class="browser-default">
              <div class="row">
                <div class="col s12 m4 l4"><li><a href="https://steamcommunity.com/sharedfiles/filedetails/?id=1470661136">JEC to DCC</a></li></div>
                <div class="col s12 m4 l4"><li><a href="https://steamcommunity.com/sharedfiles/filedetails/?id=1470668582">LITEC</a></li></div>
                <div class="col s12 m4 l4"><li><a href="#">Hub</a></li></div>
              </div>
            </ul>
            <h5>Links</h5>
            <ul class="browser-default">
              <div class="row">
                <div class="col s12 m4 l4"><li><a href="http://steamcommunity.com/id/paragonch/">Steam</a></li></div>
                <div class="col s12 m4 l4"><li><a href="src/VR_SOE/Instruction for Photogrammetry.pdf">Instruction for Photogrammetry</a></li></div>
                <div class="col s12 m4 l4"><li><a href="URS Poster.jpg">URS Poster</a></li></div>
              </div>
            </ul>
          </div>

          <div class="section">
            <h4>3D-Stitching</h4>
            <div class="divider"></div>
            <h5>People</h5>
            <ul class="browser-default">
              <li>Hanyuan Xiao</li>
              <li>Ziyu Liu</li>
              <li>Prof. <a href="https://www.ecse.rpi.edu/~rjradke/">Rich Radke</a>, ECSE (Advisor)</li>
            </ul>
            <h5>Abstract</h5>
            <p>
              In this paper we describe a method to solve a particular issue within the pipeline of 3D reconstruction of large-scale scenes. Due to the linear growing rate of processing time of each image with the number of images in data set, it is recommended to reconstruct from smaller subsets of data which are known to belong to a same object. Therefore, an automatic method to joint and merge two adjacent models is necessary to reduce the pipeline time complexity considerably. The input data in our test cases are 3D OBJ files. The method can be easily extended to other 3D model file types. Textures from original models are also recomputed to map to corresponding merged meshes. In the paper, we assume users can specify the direction in which two models are jointed and can move the surfaces to be merged to appropriate positions closed to ground truth. Output is exported in 3D OBJ type.
            </p>
            <h5>Links</h5>
            <ul class="browser-default">
              <div class="row">
                <div class="col s12 m4 l4"><li><a href="https://github.com/CorneliusHsiao/3D-Stitching">GitHub</a></li></div>
                <div class="col s12 m4 l4"><li><a href="#">LaTeX</a> (to be uploaded)</li></div>
              </div>
            </ul>
          </div>
        </div>
      </div>

      <div id="GCVA">
        <div class="container">
          <div class="section">
            <h5>People</h5>
            <ul class="browser-default">
              <li>Hanyuan Xiao</li>
              <li>Shoshana Malfatto</li>
              <li>Yanjun Li</li>
              <li>Ziniu Yu</li>
              <li>Ziyang Ji</li>
              <li>Xiuqi Li</li>
              <li>Junhao Xu</li>
              <li>Carlos Power</li>
              <li>Prof. <a href="http://www.cs.rpi.edu/~moorthy/">Mukkai S. Krishnamoorthy</a>, CSCI (Advisor)</li>
            </ul>

            <h5>Abstract</h5>
            <p>
              In this research, we investigated method to implement VR/AR on website and created 360 environment with basic hardware available. In order to achieve our final goal to establish a project with both front-end and back-end which supports everyone from the world to visit our environment, we set up three milestones during development. Within the development of the first one, we utilized 360 camera, Google Cardboard headset and other supported hardwares and found the optimal way to collect data from reality. Next, we studied the structure and usage of APIs of Aframe-in which is equipped with algorithm to bring common type of 360 videos, photos or environment onto website. To visualize our research, we wrote a program which provides users with free trip into RPI west campus. Users are enabled to select local destinations on campus or select direction to go inside 360 environments. In the last milestone, we further extended the program from the second milestone that can now navigate users into RPI campus with classical shortest-path algorithm and image data set collected.
            </p>

            <h5>Descriptions</h5>
            <p><a class="bold-text black-text">First Milestone</a></p>
            <p>One of the goals in this research is to find an optimal way for public users to establish their own VR environments and contribute to the data set worldwide such as Google Street View in VR mode. Therefore, we tested devices with the most basic functions and features available in acceptable prices in the market so far. In order to capture 360 environment, we chose to compared Samsung Gear 360, set of GoPro 4 (8 in count) and common RGB cameras. In order to select a portable tool to view VR environment, we researched and tested Google Cardboard headset. To visualize our results and make a demo, we used equipment from our selections to record a short video through west campus of RPI and published onto YouTube so that anyone with VR headset can view the video. Video can be found <a href="https://www.youtube.com/watch?v=OsOcbkqYh8s">here</a>. Further results and links can be found in <a class="italic-text" href="#GCVA-results">Result</a> section below.</p>
            <p><a class="bold-text black-text">Second Milestone</a></p>
            <p>Our team is divided into three groups to work with front-end, back-end and data collection.
              <ul class="browser-default">
                <li>Front end: Group worked with Aframe APIs and website to enable the visualization of 360 environment.</li>
                <li>Back end<a class="superscript" href="#foot-note">1</a>: Group worked with AWS since we did not have local server and server provided by school has limited size of storage.</li>
                <li>Data Collection: Group used devices selected in the first milestone to take photos around the west campus of RPI with utilization of Google Street View APIs.</li>
              </ul>
             As we successfully built VR environment and imported as a web based application, we utilized Aframe APIs and added buttons into our 360 scenes which allows users to click inside the 360 VR environment to choose direction to go with compatible controllers. Results and links can be found in <a class="italic-text" href="#GCVA-results">Result</a> section below.</p>
            <p><a class="bold-text black-text">Third Milestone</a></p>
            <p>Again, our team is divided into three groups to work with front-end, back-end and data collection. In the third milestone, we added a new feature to our program which can be used as an on-campus VR map now. We integrated classical shortest path algorithm in the front end with Google Maps APIs. Arrows to show direction inside map are calculated in real time after algorithm finds the fastest route. In data collection, we further extended our data set so more local destinations are available now. Results and links can be found in <a class="italic-text" href="#GCVA-results">Result</a> section below.</p>

            <h5>Selected Challenges</h5>
            <ul class="browser-default">
              <li>
                <p>After we set up camera and tripod on ground, it is difficult for photographers to hide within the range of controlling at some places.</p>
                <p><a class="bold-text black-text">Solution: </a>We recommend photographer to choose a large or high tripod that allows person to pouch underneath. Google Street View is also widely supported by 360 cameras such as Samsung Gear 360. Therefore, as photographer’s smartphone is online and camera is connected by Wi-Fi or Bluetooth to access network, photographer can hide at place far away and control.</p>
              </li>
              <li>
                <p>It is challenging to locate in virtual 3D coordinates when we edited our environments in Aframe. Also, it took time to align the orientation of buttons in 3D scene with the image or word on button so that user can see wherever he/she is and whichever direction he/she is facing.</p>
                <p><a class="bold-text black-text">Solution: </a>Instead of aligning pictures, we aligned all environments in orientations as they are in reality. In further data collection, we recommend photographers also set up camera to a single direction and record the exact location where the image is taken.</p>
              </li>
            </ul>

            <div id="GCVA-results">
              <h5>Results</h5>
              <p><a class="bold-text black-text">First Milestone</a></p>
              <p>
                We chose to use Samsung Gear 360 to take pictures at this time. GoPro is not suitable for static outdoor scene. If larger budget is provided or new technology is released, we would recommend the camera with higher resolution (at least 8K) and well-behaved performance in image quality. A tripod is also highly recommended. Drone or remotely controlled car is not required but can make picture capturing be easier if used.
                <ul class="browser-default">
                  <div class="row">
                    <div class="col s12 m4 l4"><li><a href="https://www.youtube.com/watch?v=OsOcbkqYh8s">360 Video</a></li></div>
                  </div>
                </ul>
              </p>
              <p><a class="bold-text black-text">Second Milestone</a></p>
              <p>
                The program is successful as we set up the demo in our presentation at RCOS<a class="superscript" href="#foot-note">2</a>. The free trip program can be found below.
                <ul class="browser-default">
                  <div class="row">
                    <div class="col s12 m6 l6"><li><a href="src/GoogleCardboardVR-AR/homepage/src/RPIVRFreeTrip/index.html">Free Trip</a> (<a class="red-text">demo under construction</a>, need local server to run<a class="superscript" href="#foot-note">2</a>)</li></div>
                  </div>
                </ul>
              </p>
              <p><a class="bold-text black-text">Third Milestone</a></p>
              <p>
                The program is successful as we set up the demo in our presentation at RCOS<a class="superscript" href="#foot-note">2</a>. The RPI VR Map program can be found below.
                <ul class="browser-default">
                  <div class="row">
                    <div class="col s12 m6 l6"><li><a href="src/GoogleCardboardVR-AR/homepage/src/RPIVRMap/index.html">RPI VR Map</a> (<a class="red-text">demo under construction</a>, need local server to run<a class="superscript" href="#foot-note">2</a>)</li></div>
                  </div>
                </ul>
              </p>
              <p><a class="bold-text black-text">Additional Files</a></p>
              <p>
                <ul class="browser-default">
                  <div class="row">
                    <div class="col s12 m4 l4"><li><a href="https://docs.google.com/presentation/d/1fQ701SIgPrkI8TtnxuUi3r4XIRN-tKAiIW3H9vYHOig/edit?usp=sharing">Presentation Slide</a></li></div>
                    <div class="col s12 m4 l4"><li><a href="#">Presentation Recording</a> (to be uploaded)</li></div>
                  </div>
                </ul>
              </p>
            </div>

            <div class="divider"></div>
            <div id="foot-note">
              <p><a class="bold-text grey-text text-darken-3">Notes</a></p>
              <ul>
                <li>1 To avoid monthly charge, we are building our server and then we can migrate data into it.</li>
                <li>2 Unfortunately, so far users need to set up local server on own computers to correctly load the program in browser. We will fix this in the future and publish on GitHub.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </main>

    <footer class="page-footer orange darken-3">
      <div class="container">
        <div class="row">
          <div class="col l6 s12">
            <h5 class="white-text">Contact</h5>
            <div><img src="src/General_info/email_research.png" alt="" class="responsive-img"></div>
            <div><img src="src/General_info/phone_research.png" alt="" class="responsive-img"></div>
          </div>
          <div class="col l4 offset-l2 s12">
            <a class="white-text">Home</a>
            <div class="divider"></div>
            <ul>
              <li><a class="white-text" href="index.html">Homepage</a></li>
              <li><a class="white-text" href="research.html">Research</a></li>
              <li><a class="white-text" href="coursework.html">Coursework</a></li>
              <li><a class="white-text" href="publication.html">Publication</a></li>
            </ul>
          </div>
        </div>
      </div>
      <div class="footer-copyright">
        <div class="container">
        Contents © 2018 Hanyuan Xiao
        </div>
      </div>
    </footer>
    
    <!--JavaScript at end of body for optimized loading-->
    <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <script type="text/javascript" src="js/materialize.min.js"></script>
    <script type="text/javascript" src="js/init.js"></script>
  </body>
</html>
